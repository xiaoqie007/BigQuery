{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat a 'Client'object\n",
    "client = bigquert.Client()\n",
    "#construct a reference to the \"openaq\" dataset\n",
    "dataset_ref = client.dataset('openaq', project = 'bigquery-public-data')\n",
    "#API request-fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "#list all the tables in the \"openaq\" dataset\n",
    "tables = list(client.list_tables(dataset))\n",
    "#Print names of all tables in the dataset(there's only one!)\n",
    "for table in tables:\n",
    "    print(table.table_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Kaggle's public dataset BigQuery integration.\n",
    "global_air_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The dataset contains only one table, called global_air_quality.\n",
    "We'll fetch the table and take a peek at the first few rows to see what sort of data it contains.\n",
    "(Again, we have hidden the code. To take a peek, click on the \"Code\" button below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a reference to the \"global_air_quality\" table\n",
    "table_ref = dataset_ref.table(\"global_air_quality\")\n",
    "# API request - fetch the table\n",
    "table = client.get_table(table_ref)\n",
    "# Preview the first five lines of the \"global_air_quality\" table\n",
    "client.list_rows(table, max_results= 5).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query to select all the items from the \"city\" column \n",
    "#where the \"country\" column is \"US\"\n",
    "query = \"\"\"\n",
    "        SELECT city\n",
    "        FROM 'bigquery-public-data.openaq.global_air_quality'\n",
    "        WHERE country = 'US'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting the query to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a \"Client\" object\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the query\n",
    "query_job = client.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next, we run the query and convert the results to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API request - run the query, and return a pandas Dataframe\n",
    "us_cities = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we've got a pandas DataFrame called us_cities, \n",
    "which we can use like any other DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What five cities have the most measurements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities.city.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query to get the score column from every row where the type column has value \"job\"\n",
    "query = \"\"\"\n",
    "        SELECT score, title\n",
    "        FROM 'bigquery-public-data.hacker_news.full'\n",
    "        WHERE type = 'job'\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a QueryJobConfig object to estimate size of query without running it\n",
    "dry_run_config = bigquery.QueryJobConfig(dry_run = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API request- dry run query to estimate costs\n",
    "dry_run_query_job = client.query(query, job_config= dry_run_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This query will process {} bytes.'.format(dry_run_query_job.total_bytes_processed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This query will process 399514186 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can also specify a parameter when running the query to limit how much data you are willing to scan. Here's an example with a low limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only run the query if it's less than 1 MB\n",
    "ONE_MB = 1000*1000\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed = ONE_MB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the query(will only run if it's less than 1 MB)\n",
    "safe_query_job = client.query(query, job_config = safe_config)\n",
    "safe_query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this case, the query was cancelled, because the limit of 1 MB was exceeded. However, we can increase the limit to run the query successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run the query if it's less than 1 GB\n",
    "ONR_GB = 1000*1000*1000\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed = ONE_GB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the query(will only run if it's less than 1 GB)\n",
    "safe_query_job = client.query(query, job_config = safe_config)\n",
    "#API request - try to run query, and return a pandas DataFrame\n",
    "job_post_scores = safe_query_job.to_dataframe()\n",
    "#Print average Score for job posts\n",
    "job_post_scores.score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to select countries with units of \"ppm\"\n",
    "!!!\n",
    "#Ê≥®ÊÑè‚ö†Ô∏è\n",
    "#`bigquery-public-data.openaq.global_air_quality`\n",
    "#‰∏äÈù¢ÊòØ‰∏çÊòØÂçïÂºïÂè∑''ÔºåËÄåÊòØ``‰∏§‰∏™ÂçïÂêëÂè∑ÔºàÂú®ÈîÆÁõòÂè≥‰∏äËßíesc‰∏ãÈù¢üëáÈÇ£‰∏™ÈîÆÔºâ\n",
    "\n",
    "first_query = \"\"\"\n",
    "              SELECT country\n",
    "              FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "              WHERE unit = \"ppm\"\n",
    "              \"\"\"\n",
    "# Your code goes here\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota, with the limit set to 10 GB)\n",
    "\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "first_query_job = client.query(first_query, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "first_results = first_query_job.to_dataframe()\n",
    "\n",
    "# View top few rows of results\n",
    "print(first_results.head())\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query to select comments that received more than 10 replies\n",
    "query_popular = \"\"\"\n",
    "                SELECT parent, COUNT(id)\n",
    "                FROM `bigquery-publio-data.hacker_news.comments`\n",
    "                GROUP BY parent\n",
    "                HAVING COUNT(id) > 10\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that our query is ready, let's run it and store the results in a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the query(cancel the query if it would use too much of\n",
    "# your quota, with the limit set to 10 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed = 10**10)\n",
    "query_job = client.query(query_popular, job_config = safe_config)\n",
    "\n",
    "# API request - run the query, and convert the results to a pandas DataFrame\n",
    "popular_comments = query_job.to_dataframe()\n",
    "\n",
    "#Print the first five rows of the DataFram\n",
    "popular_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aliasing and other improvements\n",
    "as = 'Aliasing'#Âà´Âêç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved version of earlier query, now with aliasing & improved readability\n",
    "query_improved = \"\"\"\n",
    "                SELECT parent, COUNT(1) AS NumPosts\n",
    "                FROM `bigquery-publio-data.hacker_news.comments`\n",
    "                GROUP BY parent\n",
    "                HAVING COUNT(id) > 10\n",
    "                \"\"\"\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "query_job = client.query(query_improved, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and convert the results to a pandas DataFrame\n",
    "improved_df = query_job.to_dataframe()\n",
    "\n",
    "# Print the first five rows of the DataFrame\n",
    "improved_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And this query won't work, because the author column isn't passed to an aggregate function or a GROUP BY clause:\n",
    "query_bad = \"\"\"\n",
    "            SELECT author, parent, COUNT(id)\n",
    "            FROM `bigquery-public-data.hacker_news.comments`\n",
    "            GROUP BY parent\n",
    "            \"\"\"\n",
    "#‚ö†Ô∏èÔºöif make this error, you'll get error message SELLECT list expression references\n",
    "#column (column's) which is neither grouped nor aggregated at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDER by ASC/ DESC(short for 'descending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE and DATETIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YYYY-[M]M-[D]D\n",
    "- YYYY: Four-digit year\n",
    "- [M]M: One or two digit month\n",
    "- [D]D: One or two digit day\n",
    "So 2019-01-10 is interpreted as January 10, 2019.\n",
    "The DATETIME format is like the dateformat... but with time added at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to find out the number of accidents for each day of the week\n",
    "query = \"\"\"\n",
    "        SELECT COUNT(consecutive_number) AS num_accidents,\n",
    "               EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week\n",
    "        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n",
    "        GROUP BY day_of_week\n",
    "        ORDER BY num_accidents DESC\n",
    "        \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the query(cancel the query if it would use too much of\n",
    "# your quota, with the limit set to 1 GB)\n",
    "safe_config = bigquery.QuerJobConfig(maximum_bytes_biled= 10**9)\n",
    "query_job = client.query(query, job_config= safe_config)\n",
    "\n",
    "# API request - run the query, and convert the results to a pandas DataFrame\n",
    "accidents_by_day = query_job.to_dataframe()\n",
    "\n",
    "#print the DataFrame\n",
    "accidents_by_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Requirements:\n",
    "- Your results should have the country name rather than the country code. You will have one row for each country.\n",
    "- The aggregate function for average is **AVG()**.  Use the name `avg_ed_spending_pct` for the column created by this aggregation.\n",
    "- Order the results so the countries that spend the largest fraction of GDP on education show up first.\n",
    "\n",
    "country_spend_pct_query = \"\"\"\n",
    "                          SELECT country_name, AVG(value) AS avg_ed_spending_pct\n",
    "                          FROM `bigquery-public-data.world_bank_intl_education.international_education`\n",
    "                          WHERE indicator_code = 'SE.XPD.TOTL.GD.ZS' and year >= 2010 and year <= 2017\n",
    "                          GROUP BY country_name\n",
    "                          ORDER BY avg_ed_spending_pct DESC\n",
    "                          \"\"\"\n",
    "Requirements:\n",
    "- You should have one row for each indicator code.\n",
    "- The columns in your results should be called `indicator_code`, `indicator_name`, and `num_rows`.\n",
    "- Only select codes with 175 or more rows in the raw database (exactly 175 rows would be included).\n",
    "- To get both the `indicator_code` and `indicator_name` in your resulting DataFrame, you need to include both in your **SELECT** statement (in addition to a **COUNT()** aggregation). This requires you to include both in your **GROUP BY** clause.\n",
    "- Order from results most frequent to least frequent.\n",
    "\n",
    "code_count_query = \"\"\"\n",
    "                   SELECT indicator_code, indicator_name,\n",
    "                   COUNT(1) AS num_row \n",
    "                   FROM `bigquery-public-data.world_bank_intl_education.international_education`\n",
    "                   WHERE year = 2016\n",
    "                   GROUP BY indicator_code, indicator_name\n",
    "                       HAVING COUNT(1) >= 175\n",
    "                   ORDER BY COUNT(1) DESC\n",
    "                   \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
