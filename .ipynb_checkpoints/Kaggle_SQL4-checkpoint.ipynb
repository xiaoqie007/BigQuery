{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first table is the *licenses* table\n",
    "\n",
    "from google.cloud import bigquery\n",
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()\n",
    "#Construct a reference to the \"github_repos\" dataset\n",
    "dataset_ref = client.dataset(\"github_repos\", project= \"bigquery-pubilc-data\")\n",
    "#API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "# Construct a reference to the \"licenses\" table\n",
    "licenses_ref = dataset_ref.table(\"licenses\")\n",
    "# API request - fetch the table\n",
    "licenses_table = client.get_table(licenses_ref)\n",
    "# Preview the first five lines of the \"licenses\" table\n",
    "client.list_rows(licenses_table, max_results= 5).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The second table is the *sample_files* table\n",
    "files_ref = dataseet_ref.table(\"sample_files\")\n",
    "#API request - fetch the table\n",
    "files_table = client.gettable(files_ref)\n",
    "#Preview the first five lines of the \"sample_files\" table\n",
    "client.list_rows(files_table, max_results = 5).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we write a query that uses information in both tables to determine \n",
    "#how many files are released in each license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to determine the number of files per license, sorted by number of files\n",
    "query = \"\"\"\n",
    "        SELECT L.license, COUNT(1) AS number_of_files\n",
    "        FROM `bigquery-public-data.github_repos.sample_files` AS sf\n",
    "        INTER JOIN `bigquery-public-data.github_repos.licenses` AS L\n",
    "                  ON sf.repo_name = L.repo_name\n",
    "                  \n",
    "        GROUP BY L.license\n",
    "        ORDER BY number_of_files DESC\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the query(cancel the query if it would use too much of \n",
    "#your quota, with the limit set to 10 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed = 10**10)\n",
    "query_job = client.query(query, job_config = safe_config)\n",
    "\n",
    "#API request - run the query, and convert the results to a pandas DataFrame\n",
    "file_count_by_license = query_job.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it was a big query, but it give us a nice table summarizing how many fiels have been \n",
    "# committed under each license:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the DataFrame\n",
    "file_count_by_license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You'll use *join* clauses a lot and get very efficient with them as you get some practice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Construct a reference to the \"stackoverflow\" dataset\n",
    "dataset_ref = client.dataset(\"stackoverflow\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "#2\n",
    "# Get a list of available tables \n",
    "tables = list(client.list_tables(dataset))\n",
    "list_of_tables = [table.table_id for table in tables] \n",
    "print(list_of_tables)\n",
    "\n",
    "#3\n",
    "# Construct a reference to the \"posts_answers\" table\n",
    "answers_table_ref = dataset_ref.table(\"posts_answers\")\n",
    "\n",
    "# API request - fetch the table\n",
    "answers_table = client.get_table(answers_table_ref)\n",
    "\n",
    "# Preview the first five lines of the \"posts_answers\" table\n",
    "client.list_rows(answers_table, max_results=5).to_dataframe()\n",
    "\n",
    "#4\n",
    "# Construct a reference to the \"posts_questions\" table\n",
    "questions_table_ref = dataset_ref.table(\"posts_questions\")\n",
    "\n",
    "# API request - fetch the table\n",
    "questions_table = client.get_table(questions_table_ref)\n",
    "\n",
    "# Preview the first five lines of the \"posts_questions\" table\n",
    "client.list_rows(questions_table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6) Building a more generally useful service\n",
    "\n",
    "How could you convert what you've done to a general function a website could call on the backend to get experts on any topic?  \n",
    "\n",
    "Think about it and then check the solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_finder(topic, client):\n",
    "    '''\n",
    "    Returns a DataFrame with the user IDs who have written Stack Overflow answers on a topic.\n",
    "\n",
    "    Inputs:\n",
    "        topic: A string with the topic of interest\n",
    "        client: A Client object that specifies the connection to the Stack Overflow dataset\n",
    "\n",
    "    Outputs:\n",
    "        results: A DataFrame with columns for user_id and number_of_answers. Follows similar logic to bigquery_experts_results shown above.\n",
    "    '''\n",
    "    my_query = \"\"\"\n",
    "               SELECT a.owner_user_id AS user_id, COUNT(1) AS number_of_answers\n",
    "               FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "               INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a\n",
    "                   ON q.id = a.parent_Id\n",
    "               WHERE q.tags like '%{topic}%'\n",
    "               GROUP BY a.owner_user_id\n",
    "               \"\"\"\n",
    "\n",
    "    # Set up the query (a real service would have good error handling for \n",
    "    # queries that scan too much data)\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)      \n",
    "    my_query_job = client.query(my_query, job_config=safe_config)\n",
    "\n",
    "    # API request - run the query, and return a pandas DataFrame\n",
    "    results = my_query_job.to_dataframe()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's move on to another common data format: SQL files.\n",
    "\n",
    "SQL databases are where most of the data on the web ultimately gets stored. They can be used to store data on things as simple as recipes to things as complicated as \"almost everything on the Kaggle website\".\n",
    "\n",
    "Connecting to a SQL database requires a lot more thought than reading from an Excel file. For one, you need to create a **connector**, something that will handle siphoning data from the database.\n",
    "\n",
    "`pandas` won't do this for you automatically because there are many, many different types of SQL databases out there, each with its own connector. So for a SQLite database (the only kind supported on Kaggle), you would need to first do the following (using the `sqlite3` library that comes with Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从SQL读取\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(\"../input/188-million-us-wildfires/FPA_FOD_20170508.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires = pd.read_sql_query(\"SELECT * FROM fires\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#存入SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So a CSV file is a table of values separated by commas. Hence the name: \"comma-seperated values\", or CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wine_reviews.head().to_csv(\"wine_reviews.csv\")\n",
    "\n",
    "### #To write an Excel file back you need `to_excel` and the `sheet_name` again:\n",
    "\n",
    "wic.to_excel('wic.xlsx', sheet_name='Total Women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And finally, to output to a SQL database, \n",
    "#supply the name of the table in the database we want to throw the data into, \n",
    "#and a connector:\n",
    "conn = sqlite3.connect(\"fires.sqlite\")\n",
    "fires.head(10).to_sql(\"fires\", conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
